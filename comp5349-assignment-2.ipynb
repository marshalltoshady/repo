{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This a notebook for COMP5349 Cloud Computing Assignment 2, implementing Spark Framework for data analyiss of online review data set released by Amazon on US sites. It provide various analytic phases to evaluate the data and calculate summary statistics, removing unwanted and meanning less data and finally performing some similarity analysis between review sentences.\n",
    "\n",
    "The original data include product review information of Video DVD products from Amazon between 1995 and 2015. Each row reprenst one review given by one custoer on one prodcut and containing 15 columns of informaiton. This work load has only applied anaylisis based 5 columns of informaiton: customer_id, review_id, product_id, star_rating, review_body, while the additional information are striped out.\n",
    "\n",
    "This notebook is designed to run on AWS EMR and obtain data from S3, after it's been downloaed into the bucket.\n",
    "\n",
    "The first cell contains all necessary import and initinalization, the second cell create a SparkSession, the entry point for Spark SQL Click run cell button on the menu to run the first two cells. ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "048a11000f4c48e3bec0905c27786d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5</td><td>application_1558344667099_0006</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-70-59.ec2.internal:20888/proxy/application_1558344667099_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-67-15.ec2.internal:8042/node/containerlogs/container_1558344667099_0006_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession,dataframe\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,FloatType\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pyspark.sql.functions as fc\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub \n",
    "import scipy as sp \n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1680ca0d541d4a1c856a02c5d10ff2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#start spark session for program entry  \n",
    "spark = SparkSession.builder \\\n",
    ".master(\"local[3]\") \\\n",
    ".appName(\"Comp5349 Assignment 2\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93fdab4c62ee48f583bc516373f5e1c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = \"s3://comp5349-wdai9162/amazon_reviews_us_Video_DVD_v1_00.tsv.gz\"\n",
    "schema = StructType([\n",
    "    StructField(\"marketplace\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"review_id\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_parent\", IntegerType(), True),\n",
    "    StructField(\"product_title\",StringType(),True),\n",
    "    StructField(\"product_category\",StringType(),True),\n",
    "    StructField(\"star_rating\",IntegerType(),True),\n",
    "    StructField(\"helpful_votes\",IntegerType(),True),\n",
    "    StructField(\"total_votes\",IntegerType(),True),\n",
    "    StructField(\"vine\",StringType(),True),\n",
    "    StructField(\"verified_purchase\",StringType(),True),\n",
    "    StructField(\"review_headline\",StringType(),True),\n",
    "    StructField(\"review_body\",StringType(),True),\n",
    "    StructField(\"review_date\",StringType(),True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c6eac47ec14a9fa1c2e766309b9bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#read the data according to the defined schema\n",
    "reviews = spark.read.csv(data,sep='\\t',header=True, schema=schema)\n",
    "\n",
    "#remove unrelated data column, keeping only customer_id, review_id, product_id, star_rating, review_body\n",
    "#cache it to the default storage level \n",
    "clean_reviews = reviews.drop(\"marketplace\") \\\n",
    ".drop(\"product_parent\") \\\n",
    ".drop(\"product_title\") \\\n",
    ".drop(\"product_category\") \\\n",
    ".drop(\"helpful_votes\") \\\n",
    ".drop(\"total_votes\") \\\n",
    ".drop(\"vine\") \\\n",
    ".drop(\"verified_purchase\") \\\n",
    ".drop(\"review_headline\") \\\n",
    ".drop(\"review_date\") \\\n",
    "\n",
    "#show the first 20 records, job 1 - 236.399s ,237.564s\n",
    "#clean_reviews.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stage 1.1 \n",
    "\n",
    "#total number of reviews - approx 66.0s\n",
    "total = clean_reviews.count()\n",
    "print (\"the total number of reviews is %d\" %(total))\n",
    "\n",
    "#the number of unique users - approx 84.0s \n",
    "uniq_usr = clean_reviews.dropDuplicates([\"customer_id\"]).count()\n",
    "print (\"the number of unique users is %d\" %(uniq_usr))\n",
    "\n",
    "#the number of unique products - approx 72.1s \n",
    "uniq_prdct = clean_reviews.dropDuplicates([\"product_id\"]).count()\n",
    "print (\"the number of unique products is %d\" %(uniq_prdct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stage 1.2 \n",
    "\n",
    "#the largest number of reviews published by a single user - approx 106.0s \n",
    "max_reviews_byUsr = clean_reviews.groupBy(\"customer_id\").count().agg({\"count\": \"max\"}).collect()\n",
    "print (\"the largest number of reviews published by a single user is %d\" %(max_reviews_byUsr[0]))\n",
    "\n",
    "#the top 10 users ranked by the number of reviews they publish - approx 83.0s \n",
    "sorted_list_user = clean_reviews.groupBy(\"customer_id\").count().orderBy(\"count\",ascending=[0]).show(10)\n",
    " \n",
    "#the median number of reviews published by a user - approx 85.0s \n",
    "def calc_median(column):\n",
    "    try:\n",
    "        median = np.median(column) \n",
    "        return round(float(median),2)\n",
    "    except Exception:\n",
    "        return \"input error\" #return nothing if given value is wrong \n",
    "\n",
    "median_finder = fc.udf(calc_median,FloatType()) #define function for spark datafram \n",
    "\n",
    "review_list = clean_reviews.groupBy(\"customer_id\").count().agg(fc.collect_list(\"count\").alias(\"counts\"))\n",
    "review_list = review_list.withColumn(\"median\",median_finder(\"counts\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stage 1.3 \n",
    "\n",
    "#the largest number of reviews written for a single product - approx 105.0s \n",
    "max_reviews_Product = clean_reviews.groupBy(\"product_id\").count().agg({\"count\": \"max\"}).collect()\n",
    "print (\"the largest number of reviews written for a single product %d\" %(max_reviews_Product[0]))\n",
    "max_review = max_reviews_Product[0]\n",
    "\n",
    "#the top 10 products ranked by the number of reviews they have - approx 84.0s \n",
    "sorted_products = clean_reviews.groupBy(\"product_id\").count().orderBy(\"count\",ascending=[0])\n",
    "sorted_products.show(10)\n",
    "\n",
    "#the median number of reviews a product has\n",
    "product_list = clean_reviews.groupBy(\"product_id\").count().agg(fc.collect_list(\"count\").alias(\"counts\"))\n",
    "product_list = product_list.withColumn(\"median\",median_finder(\"counts\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0007169cef498b90efc0a28d90feae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4016567"
     ]
    }
   ],
   "source": [
    "#Stage 2.1 \n",
    "\n",
    "median_row=[]\n",
    "def getmedian (median):\n",
    "    listnum=[median[i] for i in range(len(median))]\n",
    "    listnum.sort()\n",
    "    lnum=len(median)\n",
    "    if(lnum%2==1):\n",
    "        i=int((lnum+1)/2)-1\n",
    "        return listnum[i]\n",
    "    else:\n",
    "        i=int(lnum/2)-1\n",
    "        return (listnum[i] + listnum[i + 1]) / 2\n",
    "    \n",
    "# reviews with less than two sentences in the review_body - approx 1241s \n",
    "clean_reviews_filter = clean_reviews.dropna('all',None,['review_body']).rdd.filter(lambda row:(len(sent_tokenize(row.review_body)))>=2)\n",
    "clean_reviews_filter.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1236eae3ca40678cebd28d33ed4ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#reviews published by users with less than median number of reviews published approx - 1479.639s \n",
    "f=clean_reviews_filter.groupBy(lambda row:(row.customer_id)).countByKey()\n",
    "\n",
    "#clean_reviews_filter_2 ===== FINAL RESULT \n",
    "clean_reviews_filter_2=clean_reviews_filter.map(lambda row:(row.customer_id,f[row.customer_id]))\n",
    "\n",
    "\n",
    "new_list=[]\n",
    "for i in range(clean_reviews_filter_2.count()):\n",
    "    new_list.append(clean_reviews_filter_2.collect()[i][1])\n",
    "    \n",
    "num_median_reviewC=getmedian(new_list)\n",
    "\n",
    "clean_reviews_filter_2.filter(lambda row :row[1]>num_median_review)\n",
    "new_list1=[]\n",
    "for i in range(clean_reviews_filter_2.count()):\n",
    "    new_list1.append(clean_reviews_filter_2.collect()[i][0])\n",
    "clean_reviews_filter_2.filter(lambda row:row.customer_id!=[new_list1[i] for i in len(new_list1)])\n",
    "\n",
    "# new_rating2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews from products with less than median number of reviews received\n",
    "g=clean_reviews_filter_2.groupBy(lambda row:row.product_id).countByKey()\n",
    "new_rating4=clean_reviews_filter_2.map(lambda row:(row.product_id,g[row.product_id]))\n",
    "# new_rating4.collect()[0][1]\n",
    "new_list2=[]\n",
    "for i in range(new_rating4.count()):\n",
    "    new_list2.append(new_rating4.collect()[i][1])\n",
    "\n",
    "num_median_reviewP=getmedian(new_list2)\n",
    "\n",
    "new_rating4.filter(lambda row:row[1]>num_median_reviewP)\n",
    "\n",
    "new_list3=[]\n",
    "for i in range(new_rating4.count()):\n",
    "    new_list3.append(new_rating4.collect()[i][0])\n",
    "\n",
    "clean_reviews_filter_2.filter(lambda row: row.product_id!=[new_list3[i] for i in len(new_list3)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stage 3: Similarity analysis with Sentence Embedding\n",
    "\n",
    "#5.1 Positive vs. Negative Reviews\n",
    "\n",
    "#pick the 10th most popular prodcut by review number according to result calculated from stage 1.3 - approx 95.0s \n",
    "pop_product_10th = sorted_products.select(\"product_id\").collect()[9][0]\n",
    "stage3_data = clean_reviews.filter(clean_reviews.product_id == pop_product_10th) \n",
    "\n",
    "positive_class = stage3_data.filter(stage3_data.star_rating >= 4)\n",
    "positive_review = positive_class.select(\"review_body\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "negative_class = stage3_data.filter(stage3_data.star_rating <= 2)\n",
    "negative_review = negative_class.select(\"review_body\").rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" \n",
    "embed = hub.Module(module_url) \n",
    "\n",
    "#sentence segamentation for both positive lists \n",
    "positive_review_sent = [] \n",
    "\n",
    "for i in range (len(positive_review)):\n",
    "    positive_review_sent.append(sent_tokenize(positive_review[i]))\n",
    "    \n",
    "f_positive_sent = [y for x in positive_review_sent for y in x] #flat the list \n",
    "\n",
    "#sentence segamentation for both negative lists \n",
    "negative_review_list = [] \n",
    "for i in range (len(negative_review)):\n",
    "    negative_review_list.append(sent_tokenize(negative_review[i]))\n",
    "\n",
    "f_negative_sent = [y for x in negative_review_list for y in x] #flat the list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(positive_review))\n",
    "print (len(positive_review_sent ))\n",
    "print (len(f_positive_sent))\n",
    "print (f_positive_sent)\n",
    "\n",
    "#filter \"!\",\"?\", clean data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Google Pre-trained universal sentence encoder to encode each sentence to 512 dimension vector \n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    sentence_embeddings_pos = session.run(embed(f_positive_sent))\n",
    "    sentence_embeddings_neg = session.run(embed(f_negative_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (sentence_embeddings_pos.shape)\n",
    "print (sentence_embeddings_neg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.2 Intra-Class Similarity, 0 = similar, 2 = different/opposite \n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "#calculate pairwise distances between observations in n-dimensional space.\n",
    "dist_matrix_positive = pdist(sentence_embeddings_pos,\"cosine\")\n",
    "dist_matrix_negative = pdist(sentence_embeddings_neg,\"cosine\")\n",
    "\n",
    "average_dist_positive = np.mean(dist_matrix_positive)\n",
    "average_dist_negative = np.mean(dist_matrix_negative)\n",
    "\n",
    "print (average_dist_positive)\n",
    "print (average_dist_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate class center \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
